package vllm

import (
	"fmt"
	"log"
	"os"
	"os/exec"
	"strconv"
	"strings"
	"syscall"
)

type Config struct {
	// æ¨¡å‹æ–‡ä»¶path
	ModelPath string
	// buildingåœ°å€ â†’ ç›‘å¬åœ°å€ï¼Œ é»˜è®¤0.0ã€‚0.0
	Host string
	// éƒ¨é—¨åœ°å€ â†’ ç›‘å¬ç«¯å£
	Port int

	// GPU å¹¶è¡Œçš„æ•°é‡ --tensor-parallel-size
	TensorParallelSize int
	// æ˜¾å¡åˆ©ç”¨ç‡ï¼ˆ0-1.0ï¼‰ --gpu-memory-utilization
	GPUMemoryUtilization float64
	// æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œ 0 å°±æ˜¯é»˜è®¤ï¼› --max-model-len
	MaxModelLen int
	// data typeï¼Œ
	Dtype string
	// å…œåº•å‡½æ•°ï¼Œç”¨äºä¼ é€’ä»»æ„å…¶ä»–å‚æ•°
	ExtraArgs []string
}

// åˆå§‹åŒ–configï¼Œ å¡«å†™defaultå€¼
func DefaultConfig(modelPath string) *Config {
	return &Config{
		ModelPath:            modelPath,
		Host:                 "0.0.0.0",
		Port:                 8000,
		TensorParallelSize:   1,
		GPUMemoryUtilization: 0.9,
		Dtype:                "auto",
	}
}

// å¾ˆç®€å•å°±æ˜¯å¾€ vLLM config é‡Œé¢å¡«å†™data çš„
func LoadConfigFromEnv(modelPath string) *Config {
	config := DefaultConfig(modelPath)

	if v := os.Getenv("VLLM_HOST"); v != "" {
		config.Host = v
	}
	if v := os.Getenv("VLLM_PORT"); v != "" {
		if port, err := strconv.Atoi(v); err == nil {
			config.Port = port
		}
	}
	if v := os.Getenv("VLLM_TENSOR_PARALLEL_SIZE"); v != "" {
		if tp, err := strconv.Atoi(v); err == nil {
			config.TensorParallelSize = tp
		}
	}
	if v := os.Getenv("VLLM_GPU_MEMORY_UTILIZATION"); v != "" {
		if gpu, err := strconv.ParseFloat(v, 64); err == nil {
			config.GPUMemoryUtilization = gpu
		}
	}
	if v := os.Getenv("VLLM_MAX_MODEL_LEN"); v != "" {
		if maxLen, err := strconv.Atoi(v); err == nil {
			config.MaxModelLen = maxLen
		}
	}
	if v := os.Getenv("VLLM_DTYPE"); v != "" {
		config.Dtype = v
	}
	if v := os.Getenv("VLLM_EXTRA_ARGS"); v != "" {
		config.ExtraArgs = strings.Fields(v)
	}

	return config
}

type Server struct {
	config *Config
	cmd    *exec.Cmd
}

// è¿™é‡Œç”¨config å°±å¯ä»¥initalized Server model
func NewServer(config *Config) *Server {
	return &Server{config: config}
}

// æŠŠ config é‡Œé¢çš„ä¸œè¥¿è½¬åŒ–æˆ cmd ç»™vllm
func (s *Server) buildArgs() []string {
	args := []string{
		"-m", "vllm.entrypoints.openai.api_server",
		"--model", s.config.ModelPath,
		"--host", s.config.Host,
		"--port", strconv.Itoa(s.config.Port),
		"--tensor-parallel-size", strconv.Itoa(s.config.TensorParallelSize),
		"--gpu-memory-utilization", fmt.Sprintf("%.2f", s.config.GPUMemoryUtilization),
		"--dtype", s.config.Dtype,
	}

	if s.config.MaxModelLen > 0 {
		args = append(args, "--max-model-len", strconv.Itoa(s.config.MaxModelLen))
	}
	if len(s.config.ExtraArgs) > 0 {
		args = append(args, s.config.ExtraArgs...)
	}

	return args
}

// æ•´ä½“é€»è¾‘ï¼Œç»™vllm server çš„cmd è¡¥å…¨
func (s *Server) Start() error {
	args := s.buildArgs()
	log.Printf("ğŸš€ Starting vLLM: python %s", strings.Join(args, " "))

	s.cmd = exec.Command("python", args...)
	s.cmd.Stdout = os.Stdout
	s.cmd.Stderr = os.Stderr

	if err := s.cmd.Start(); err != nil {
		return fmt.Errorf("Failed to start vLLM: %w", err)
	}
	log.Printf("âœ… vLLM started with PID %d", s.cmd.Process.Pid)
	return nil
}

// ä¸¤ä¸ªè¾…åŠ©å‡½æ•°ï¼Œä¸€ä¸ªåœæ­¢ï¼Œä¸€ä¸ªè¡¥å…¨
func (s *Server) Wait() error {
	if s.cmd == nil {
		return fmt.Errorf("vLLM not started")
	}
	return s.cmd.Wait()
}
func (s *Server) Stop() error {
	if s.cmd == nil || s.cmd.Process == nil {
		return nil
	}
	return s.cmd.Process.Signal(syscall.SIGTERM)
}
